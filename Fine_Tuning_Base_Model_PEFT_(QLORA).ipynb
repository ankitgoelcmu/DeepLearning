{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPgBUcWXAxoB02zONl1i08Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitgoelcmu/DeepLearning/blob/main/Fine_Tuning_Base_Model_PEFT_(QLORA).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In this Notebook - I will be doing threat detection using the Mireu-Lab/NSL-KDD dataset (a benchmark for network intrusion detection with binary classes: Normal, Anamoly)\n",
        "### This guide fine-tunes DistilBERT (lightweight text model) by converting tabular features to log-like strings (e.g., \"duration=0 protocol=tcp src_bytes=215\"), optimizes with QLoRA (4-bit), quantizes/prunes for speed.\n",
        "\n",
        "## I will be using Hugging face for the dataset + base model + PEFT.\n",
        "\n",
        "## TODO Deploy to Gradio"
      ],
      "metadata": {
        "id": "9J7MHSUrSUL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hugging Face Cheat Sheet (Quick Refresher)\n",
        "\n",
        "1. Transformers: Models/tokenizers (e.g., AutoModelForSequenceClassification).\n",
        "2. Datasets: load_dataset('enron_spam')—easy data.\n",
        "3. PEFT/LoRA: LoraConfig(r=8)—adapters for efficient FT.\n",
        "4. BitsAndBytes: BitsAndBytesConfig(load_in_4bit=True)—quantization.\n",
        "5. Trainer: Trainer(model, args, dataset)—handles loops/metrics.\n",
        "6. Gradio: gr.Interface(fn=predict, ...).launch()—UI demo.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wObfQzmMT7uN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1: Loading dataset and exploring its features, labels, class etc"
      ],
      "metadata": {
        "id": "ijk1PsmhZ0Mq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-KDaknwRTh7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "train_dataset = load_dataset(\"Mireu-Lab/NSL-KDD\", split=\"train\")\n",
        "test_dataset  = load_dataset(\"Mireu-Lab/NSL-KDD\", split=\"test\")\n",
        "\n",
        "# Lets select 2000 random dataset values by shuffling the dataset and then selecting the first 200 elements.\n",
        "train_dataset = train_dataset.shuffle().select(range(4000))\n",
        "test_dataset = test_dataset.shuffle().select(range(200))\n",
        "\n",
        "print(f\"train_dataset size: {len(train_dataset)}\")\n",
        "print(f\"test_dataset size: {len(test_dataset)}\")\n",
        "print(\"First 5 samples from the randomly selected 200:\")\n",
        "print(train_dataset[:5])"
      ],
      "metadata": {
        "id": "yKRsq3F0Z2-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inspect the first dataset\n",
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "E0UUHEDMaPD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.column_names"
      ],
      "metadata": {
        "id": "oEDWgQGbaSHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_integers = random.sample(range(len(train_dataset)), 5)\n",
        "train_dataset[random_integers]"
      ],
      "metadata": {
        "id": "5A-aHa5oa0zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.unique(\"class\")"
      ],
      "metadata": {
        "id": "TF52as6kbuk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking count of  labels\n",
        "from collections import Counter\n",
        "\n",
        "Counter(train_dataset[\"class\"])"
      ],
      "metadata": {
        "id": "oBMcQUgPcGA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn our dataset into a DataFrame and get a random sample\n",
        "random_integers = random.sample(range(len(train_dataset)), 10)\n",
        "network_traffic = pd.DataFrame(train_dataset[random_integers])\n",
        "network_traffic"
      ],
      "metadata": {
        "id": "SNmVBkSUcf4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_traffic[\"class\"].value_counts()"
      ],
      "metadata": {
        "id": "t3k-f_Y0cf64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a mapping from Class/Labels to numbers [0 -> Normal, 1 -> Anomaly]"
      ],
      "metadata": {
        "id": "sHIrmd6Di6ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mapping from id2label and label2id\n",
        "id2label = {0: \"normal\", 1: \"anomaly\"}\n",
        "label2id = {\"normal\": 0, \"anomaly\": 1}\n",
        "\n",
        "print(f\"Label to ID mapping: {label2id}\")\n",
        "print(f\"ID to Label mapping: {id2label}\")\n",
        "\n",
        "\n",
        "#Now turn our Class/Labels in 0 or 1\n",
        "\n",
        "def map_class_to_id(example):\n",
        "    example[\"class\"] = label2id[example[\"class\"]]\n",
        "    return example\n",
        "\n",
        "#Now Test this function with our train_dataset[0]\n",
        "map_class_to_id(train_dataset[0])"
      ],
      "metadata": {
        "id": "9d1Y97-yjARj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now lets convert Class to IDs for or whole data set - train + test\n",
        "train_dataset = train_dataset.map(map_class_to_id)\n",
        "test_dataset  = test_dataset.map(map_class_to_id)\n",
        "\n",
        "train_dataset.features[\"class\"]"
      ],
      "metadata": {
        "id": "Lvm_yQ7rkmkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "PVcRxVGWmj6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess: Convert Tablular Features to Text Strings and store the String in \"text\" Column. This will alllow us to tokenize the \"text\"\n",
        "`(e.g = \"duration=0 protocol=tcp ...\"`"
      ],
      "metadata": {
        "id": "N5ox_nKNm6qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tabular_to_text(examples):\n",
        "  #Tabular example is dict with keys like 'duration', 'protocol_type', 'class'\n",
        "  texts = []\n",
        "  for i in range(len(examples[\"duration\"])):\n",
        "      text = (\n",
        "            f\"duration={examples['duration'][i]} \"\n",
        "            f\"protocol_type={examples['protocol_type'][i]} \"\n",
        "            f\"service={examples['service'][i]} \"\n",
        "            f\"flag={examples['flag'][i]} \"\n",
        "            f\"src_bytes={examples['src_bytes'][i]} \"\n",
        "            f\"dst_bytes={examples['dst_bytes'][i]} \"\n",
        "            f\"land={examples['land'][i]} \"\n",
        "            f\"wrong_fragment={examples['wrong_fragment'][i]} \"\n",
        "            f\"urgent={examples['urgent'][i]} \"\n",
        "            f\"hot={examples['hot'][i]} \"\n",
        "            f\"num_failed_logins={examples['num_failed_logins'][i]} \"\n",
        "            f\"logged_in={examples['logged_in'][i]} \"\n",
        "            f\"num_compromised={examples['num_compromised'][i]} \"\n",
        "            f\"root_shell={examples['root_shell'][i]} \"\n",
        "            f\"su_attempted={examples['su_attempted'][i]} \"\n",
        "            f\"num_root={examples['num_root'][i]} \"\n",
        "            f\"num_file_creations={examples['num_file_creations'][i]} \"\n",
        "            f\"num_shells={examples['num_shells'][i]} \"\n",
        "            f\"num_access_files={examples['num_access_files'][i]} \"\n",
        "            f\"num_outbound_cmds={examples['num_outbound_cmds'][i]} \"\n",
        "            f\"is_host_login={examples['is_host_login'][i]} \"\n",
        "            f\"is_guest_login={examples['is_guest_login'][i]} \"\n",
        "            f\"count={examples['count'][i]} \"\n",
        "            f\"srv_count={examples['srv_count'][i]} \"\n",
        "            f\"serror_rate={examples['serror_rate'][i]} \"\n",
        "            f\"srv_serror_rate={examples['srv_serror_rate'][i]} \"\n",
        "            f\"rerror_rate={examples['rerror_rate'][i]} \"\n",
        "            f\"srv_rerror_rate={examples['srv_rerror_rate'][i]} \"\n",
        "            f\"same_srv_rate={examples['same_srv_rate'][i]} \"\n",
        "            f\"diff_srv_rate={examples['diff_srv_rate'][i]} \"\n",
        "            f\"srv_diff_host_rate={examples['srv_diff_host_rate'][i]} \"\n",
        "            f\"dst_host_count={examples['dst_host_count'][i]} \"\n",
        "            f\"dst_host_srv_count={examples['dst_host_srv_count'][i]} \"\n",
        "            f\"dst_host_same_srv_rate={examples['dst_host_same_srv_rate'][i]} \"\n",
        "            f\"dst_host_diff_srv_rate={examples['dst_host_diff_srv_rate'][i]} \"\n",
        "            f\"dst_host_same_src_port_rate={examples['dst_host_same_src_port_rate'][i]} \"\n",
        "            f\"dst_host_srv_diff_host_rate={examples['dst_host_srv_diff_host_rate'][i]} \"\n",
        "            f\"dst_host_serror_rate={examples['dst_host_serror_rate'][i]} \"\n",
        "            f\"dst_host_srv_serror_rate={examples['dst_host_srv_serror_rate'][i]} \"\n",
        "            f\"dst_host_rerror_rate={examples['dst_host_rerror_rate'][i]} \"\n",
        "            f\"dst_host_srv_rerror_rate={examples['dst_host_srv_rerror_rate'][i]} \"\n",
        "        )\n",
        "      texts.append(text)\n",
        "  return {'text': texts}"
      ],
      "metadata": {
        "id": "BGdGiH5inXm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply to dataset\n",
        "train_dataset = train_dataset.map(tabular_to_text, batched=True)\n",
        "test_dataset = test_dataset.map(tabular_to_text, batched=True)\n",
        "\n",
        "#Lets check the newly inserted \"text\"\n",
        "train_dataset[0][\"text\"]"
      ],
      "metadata": {
        "id": "iXeEnSD8qSUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "Aq6D6gdAtj_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "tokenizer"
      ],
      "metadata": {
        "id": "qyhHBLuXg9id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size\n",
        "tokenizer(\"I am Ankit Goel\")"
      ],
      "metadata": {
        "id": "tMjX4uIGhzoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length"
      ],
      "metadata": {
        "id": "YeOfGdiXiLPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenize the text logs"
      ],
      "metadata": {
        "id": "6uH6-3g4sG7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(examples):\n",
        "  return tokenizer(examples[\"text\"],\n",
        "                   padding=True,\n",
        "                   truncation=True)\n",
        "\n",
        "\n",
        "# Map our tokenize_text function to the dataset, remove all the columns except 'class'\n",
        "# Final columns = 'input_ids', 'attention_mask'(from tokenize) + 'class'\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_text, batched=True, batch_size=1000,\n",
        "                                            remove_columns=[col for col in train_dataset.column_names if col != 'class'])\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_text, batched=True, batch_size=1000,\n",
        "                                            remove_columns=[col for col in train_dataset.column_names if col != 'class'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "coz_K5IktMiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset\n"
      ],
      "metadata": {
        "id": "Rl3wHFRjzk60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get two samples from the tokenized dataset\n",
        "train_tokenized_sample = tokenized_train_dataset[0]\n",
        "test_tokenized_sample = tokenized_test_dataset[0]\n",
        "\n",
        "for key in train_tokenized_sample.keys():\n",
        "    print(f\"[INFO] Key: {key}\")\n",
        "    print(f\"Train sample: {train_tokenized_sample[key]}\")\n",
        "    print(f\"Test sample: {test_tokenized_sample[key]}\")\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "d_WrVK_c38M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now time to format Train + Test dataset for PyTorch compatibility\n",
        "tokenized_test_dataset = tokenized_test_dataset.rename_column('class', 'labels')\n",
        "tokenized_train_dataset = tokenized_train_dataset.rename_column('class', 'labels')\n",
        "tokenized_test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "tokenized_train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ],
      "metadata": {
        "id": "CKntKQQj4gc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify (label preserved!)\n",
        "print(f\"Train columns: {tokenized_test_dataset.column_names}\")  # ['input_ids', 'attention_mask', 'class']\n",
        "print(f\"Sample label: {tokenized_test_dataset[0]['labels']}\")  # e.g., 0 (normal)\n",
        "print(f\"Sample input_ids len: {len(tokenized_test_dataset[0]['input_ids'])}\")"
      ],
      "metadata": {
        "id": "kQS6PKhWz3tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting up an evaluation metric"
      ],
      "metadata": {
        "id": "MQ0SPL4K5xnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\") # Load the f1 metric\n",
        "\n",
        "def compute_accuracy(predictions_and_labels: Tuple[np.array, np.array]):\n",
        "  \"\"\"\n",
        "  Computes the accuracy and F1-score of a model by comparing the predictions and labels.\n",
        "  \"\"\"\n",
        "  predictions, labels = predictions_and_labels\n",
        "\n",
        "  # Get highest prediction probability of each prediction if predictions are probabilities\n",
        "  if len(predictions.shape) >= 2:\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "  accuracy_result = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "  f1_result = f1_metric.compute(predictions=predictions, references=labels, average=\"binary\") # Calculate F1 for binary classification\n",
        "\n",
        "  return {**accuracy_result, **f1_result} # Return both metrics"
      ],
      "metadata": {
        "id": "WE_TRv2z5zVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets test it out\n",
        "# Create example list of predictions and labels\n",
        "example_predictions_all_correct = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "example_predictions_one_wrong = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
        "example_labels = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "\n",
        "# Test the function\n",
        "print(f\"Accuracy when all predictions are correct: {compute_accuracy((example_predictions_all_correct, example_labels))}\")\n",
        "print(f\"Accuracy when one prediction is wrong: {compute_accuracy((example_predictions_one_wrong, example_labels))}\")"
      ],
      "metadata": {
        "id": "U5ODhpcB55hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_params(model):\n",
        "    \"\"\"\n",
        "    Count the parameters of a PyTorch model.\n",
        "    \"\"\"\n",
        "    trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_parameters = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    return {\"trainable_parameters\": trainable_parameters, \"total_parameters\": total_parameters}\n",
        "\n",
        "# Count the parameters of the model\n",
        "#count_params(model)\n",
        "\n"
      ],
      "metadata": {
        "id": "XaHM-yhxCnoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Dirs to Save the Model"
      ],
      "metadata": {
        "id": "shYpTaERDJbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model output directory\n",
        "from pathlib import Path\n",
        "\n",
        "# Create models directory\n",
        "models_dir = Path(\"models\")\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Create model save name\n",
        "model_save_name = \"network-threat_classifier-distilbert-base-uncased\"\n",
        "\n",
        "# Create model save path\n",
        "model_save_dir = Path(models_dir, model_save_name)\n",
        "\n",
        "model_save_dir"
      ],
      "metadata": {
        "id": "l2VKwIW1DIj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PEFT TIME!!!\n",
        "\n"
      ],
      "metadata": {
        "id": "vzlzH94pDlSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade transformers bitsandbytes accelerate peft torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "#Define Quatinzed Config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Back to 4-bit—saves 75% memory\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model with quantization (same params, now quantized)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"bert-base-uncased\", #distilbert/distilbert-base-uncased\n",
        "    quantization_config=bnb_config,\n",
        "    num_labels=2, # can customize this to the number of classes in your dataset\n",
        "    id2label=id2label, # mappings from class IDs to the class labels (for classification tasks)\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "\n",
        "config = LoraConfig(\n",
        "    task_type=\"SEQ_CLS\", # Change task_type for sequence classification\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"key\", \"value\"], # Corrected target modules for DistilBert\n",
        "    lora_dropout=0.1,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "Oc0n0pqCDkfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inspect the output of `model.print_trainable_parameters()`\n",
        "\n",
        "##trainable params: 443,906 || all params: 109,927,684 || trainable%: 0.4038\n",
        "\n",
        "##Note:\n",
        "\n",
        "1.   Total Params: ~109M (BERT-base).\n",
        "2.   Trainable: ~444k (0.4%)—only LoRA adapters + classifier head..  \n",
        "\n"
      ],
      "metadata": {
        "id": "xM2O92pEUfgI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bade9f7e"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA is available! Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Using CPU.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "xdT9RsKWJEnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TODO : Add text to explore above model"
      ],
      "metadata": {
        "id": "pDp7B9r3WW_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Training Arguments with TrainingArguments"
      ],
      "metadata": {
        "id": "iWwKEzl8XJo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments (\n",
        "    output_dir=model_save_dir,\n",
        "    learning_rate=2e-3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    use_cpu=False,\n",
        "    push_to_hub=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    weight_decay=0.01,\n",
        "    #class_weight='balanced',\n",
        "    logging_steps=1, # Log every step to see the training loss for small dataset\n",
        "    report_to=[] # Explicitly disable external reporting to ensure console output\n",
        ")"
      ],
      "metadata": {
        "id": "zZtLF3eEWghi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args"
      ],
      "metadata": {
        "id": "p-YTXqzRZ_xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up an instance of Trainer\n"
      ],
      "metadata": {
        "id": "1eqom_pZaDCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_accuracy)"
      ],
      "metadata": {
        "id": "cH0_ylq5aHTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets TRAIN TRAIN TRAIN\n",
        "\n",
        "results = trainer.train()"
      ],
      "metadata": {
        "id": "fd4Xqvdsaiy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qlora_results = trainer.evaluate()\n",
        "qlora_acc = qlora_results['eval_accuracy']\n",
        "qlora_f1 = qlora_results['eval_f1'] # This will now be available\n",
        "print(f\"QLoRA Acc: {qlora_acc:.3f}, F1: {qlora_f1:.3f}\")"
      ],
      "metadata": {
        "id": "taEDMqJCkAzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect training metrics\n",
        "for key, value in results.metrics.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "4_ommBVpd07I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the Model\n",
        "print(f\"[INFO] Saving model to {model_save_dir}\")\n",
        "trainer.save_model(output_dir=model_save_dir)"
      ],
      "metadata": {
        "id": "6jScQAOhd58G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get training history\n",
        "trainer_history_all = trainer.state.log_history\n",
        "trainer_history_all[:4]"
      ],
      "metadata": {
        "id": "uA69CCWweVvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "756754c1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Extract training and evaluation loss from trainer_history_all\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "epochs = []\n",
        "\n",
        "for log_entry in trainer_history_all:\n",
        "    if 'loss' in log_entry: # Training loss\n",
        "        train_losses.append(log_entry['loss'])\n",
        "        epochs.append(log_entry['epoch'])\n",
        "    if 'eval_loss' in log_entry: # Evaluation loss\n",
        "        eval_losses.append(log_entry['eval_loss'])\n",
        "\n",
        "# Adjust epochs for eval_losses to match the number of eval steps, not all training steps\n",
        "# The number of eval_losses should correspond to the number of eval_steps, which are per epoch.\n",
        "# We can use a simpler approach by aligning them based on when they appear in the history.\n",
        "\n",
        "# Filter for entries that have 'loss' (training loss) and 'eval_loss' (validation loss)\n",
        "training_logs = [entry for entry in trainer_history_all if 'loss' in entry and 'learning_rate' in entry]\n",
        "evaluation_logs = [entry for entry in trainer_history_all if 'eval_loss' in entry]\n",
        "\n",
        "train_epochs = [entry['epoch'] for entry in training_logs]\n",
        "train_losses = [entry['loss'] for entry in training_logs]\n",
        "\n",
        "eval_epochs = [entry['epoch'] for entry in evaluation_logs]\n",
        "eval_losses = [entry['eval_loss'] for entry in evaluation_logs]\n",
        "\n",
        "# Plotting the loss curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x=train_epochs, y=train_losses, label='Training Loss')\n",
        "sns.lineplot(x=eval_epochs, y=eval_losses, label='Validation Loss')\n",
        "\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}