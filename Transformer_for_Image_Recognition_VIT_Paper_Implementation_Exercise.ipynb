{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitgoelcmu/DeepLearning/blob/main/Transformer_for_Image_Recognition_VIT_Paper_Implementation_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNqPNlYylluR"
      },
      "source": [
        "# In this Notebook - I am replicating the VIT Paper - [AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929)\n",
        "\n",
        "In this notebook - I will be building visiion tranformer model as layed out by the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this notebook -  I am implenting 2 iterations of this ViT Model\n",
        "\n",
        "## First Iteration - I am planning to use Pytorch OOB function `TransformerEncoderLayer` https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html\n",
        "\n",
        "## Second Iteration -I will implemnent Multi Head Block and MLP Block. Represented by Equation 2 and 3 of the Paper. And then use those 2 building blocks to create Transformer Layer."
      ],
      "metadata": {
        "id": "cfd72fPnHTL3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf8ab9cyHTzU"
      },
      "source": [
        "### Get various imports and helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChRaHUSJ8DYZ"
      },
      "outputs": [],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5H5P8EjCNGK"
      },
      "outputs": [],
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the `device`"
      ],
      "metadata": {
        "id": "IsEleemcAdWc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE1AAH_uCjiP"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xlCriwBTAch3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmS5yuvxCpLp"
      },
      "source": [
        "# Get data\n",
        "\n",
        "Want to download the data we've been using in PyTorch Paper Replicating: https://www.learnpytorch.io/08_pytorch_paper_replicating/#1-get-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm772wqgCzN9"
      },
      "outputs": [],
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1ML2c-dCzCi"
      },
      "outputs": [],
      "source": [
        "# Setup directory paths to train and test images\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNBZ_2h_Cy86"
      },
      "source": [
        "### Preprocess data\n",
        "\n",
        "Turn images into tensors. Table 3 of the paper mentions image resolution to be 224 - `Training resolution is 224.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mU0T4gP3DJdF"
      },
      "outputs": [],
      "source": [
        "# Create image size (from Table 3 in the ViT paper )\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Create transform pipeline manually\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "print(f\"Manually created transforms: {manual_transforms}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4vWgIprDJau"
      },
      "outputs": [],
      "source": [
        "# Set the batch size\n",
        "# Note ViT paper mentions (under Table 3) = All models are trained with a batch size of 4096.\n",
        "# But for our model, we will start small and use batch size = 32\n",
        "BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n",
        "\n",
        "# Create Data Loaders. The DataLoader provides features like automatic batching, shuffling, and multiprocessing for efficient data loading.\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms, # use manually created transforms\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7eLIFHyDJRr"
      },
      "outputs": [],
      "source": [
        "# Get a batch of images\n",
        "image_batch, label_batch = next(iter(train_dataloader))\n",
        "\n",
        "# Get a single image from the batch\n",
        "image, label = image_batch[0], label_batch[0]\n",
        "\n",
        "# View the batch shapes\n",
        "image.shape, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yyNHCmCDbSR"
      },
      "outputs": [],
      "source": [
        "# Plot image with matplotlib\n",
        "plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwmoMhW8IqSu"
      },
      "source": [
        "## 1. For this ViT architecture I will use in-built [PyTorch transformer layers](https://pytorch.org/docs/stable/nn.html#transformer-layers).\n",
        "\n",
        "* You'll want to look into replacing our `TransformerEncoderBlock()` class with [`torch.nn.TransformerEncoderLayer()`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer) (these contain the same layers as our custom blocks).\n",
        "* You can stack `torch.nn.TransformerEncoderLayer()`'s on top of each other with [`torch.nn.TransformerEncoder()`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmDd_YZ7VSrL"
      },
      "outputs": [],
      "source": [
        "# TODO: Ankit\n",
        "class ViT (nn.Module):\n",
        "  def __init__ (self,\n",
        "                img_size: int=224,\n",
        "                patch_size: int=16,\n",
        "                in_channels: int=3,\n",
        "                num_classes: int=1000,\n",
        "                mlp_size: int=3072,\n",
        "                num_heads: int=12,\n",
        "                num_encooder_layer: int=12,\n",
        "                embedding_dim: int=768,\n",
        "                attn_dropout: float=0,\n",
        "                mlp_dropput: float=0.1,\n",
        "                embedding_dropout: float=0.1,\n",
        "                num_out_class: int=1000):\n",
        "    super().__init__()\n",
        "\n",
        "    assert img_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "    self.num_patches = int((img_size // patch_size) ** 2)\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "    self.embedding_class = nn.Parameter(torch.zeros(1, 1, embedding_dim))\n",
        "    self.embedding_position = nn.Parameter(torch.zeros(1, 1 + self.num_patches, embedding_dim))\n",
        "    self.embedding_dropout = nn.Dropout(embedding_dropout)\n",
        "    #Using Convolution to create Patch Embeddings\n",
        "    self.patch = nn.Conv2d(in_channels=in_channels,\n",
        "                                    out_channels=embedding_dim,\n",
        "                                    kernel_size=patch_size,\n",
        "                                    stride=patch_size)\n",
        "    #flattening the patches\n",
        "    self.flatten_patches = nn.Flatten (start_dim=2, end_dim=3)\n",
        "    self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim,\n",
        "                                                    nhead=num_heads,\n",
        "                                                    dim_feedforward=mlp_size,\n",
        "                                                    dropout=attn_dropout,\n",
        "                                                    activation='gelu',\n",
        "                                                    batch_first=True,\n",
        "                                                    norm_first=True)  #batch, seq, feature. --> image batch, patch seq, embidding features\n",
        "    self.encoder = nn.TransformerEncoder(encoder_layer=self.encoder_layer,\n",
        "                                         num_layers=num_encooder_layer)\n",
        "\n",
        "    self.classifier = nn.Sequential(nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "                                    nn.Linear(in_features=embedding_dim, out_features=num_out_class)\n",
        "                                    )\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    x = self.patch (x)\n",
        "    x = self.flatten_patches(x)\n",
        "    x = x.permute(0, 2, 1)\n",
        "    class_embedding = self.embedding_class.expand(batch_size, -1, -1)\n",
        "    x = torch.cat([class_embedding, x], dim=1)\n",
        "    x = self.embedding_position + x\n",
        "    x = self.embedding_dropout(x)\n",
        "    x = self.encoder(x)\n",
        "    x = self.classifier(x[:, 0])\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)\n",
        "vit = ViT(num_out_class=3)\n",
        "vit(random_image_tensor)"
      ],
      "metadata": {
        "id": "d_QfH3hf9hg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# # Print a summary of our custom ViT model using torchinfo (uncomment for actual output)\n",
        "summary(model=vit,\n",
        "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "id": "ca_a-zfdCw2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random tensor with same shape as a single image\n",
        "random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)\n",
        "embedding_class = nn.Parameter(torch.zeros(1, 1, 768))\n",
        "embedding_class.shape\n",
        "patch_features_oo = nn.Conv2d(in_channels=3,\n",
        "                                    out_channels=768,\n",
        "                                    kernel_size=16,\n",
        "                                    stride=16)\n"
      ],
      "metadata": {
        "id": "urTvXeNT31gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "results = engine.train(model=vit,\n",
        "                       train_dataloader=train_dataloader,\n",
        "                       test_dataloader=test_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       loss_fn=loss_fn,\n",
        "                       epochs=10,\n",
        "                       device=device)"
      ],
      "metadata": {
        "id": "kx9cLGEzDKEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(results)"
      ],
      "metadata": {
        "id": "i5MhYGzn4doj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### So far we have used Pytorch OOB Layer `TransformerEncoderLayer` to implement Encoder layer of the ViT transformer.\n"
      ],
      "metadata": {
        "id": "nPojX1reaCIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this step we will implement Encode Layer with Multi Head Block and MLP Block.\n",
        "\n",
        "### `Multi Head Block `= `Layer Norm(Embedded Patches) --> Multi Head Attention + Embedded Patches (Residual Connection) + Dropout `\n",
        "\n",
        "#### **Residual Connection** - Residual connections help stabilize neural network training by providing a direct pathway for information and gradient flow during the forward pass and backpropagation."
      ],
      "metadata": {
        "id": "FyPOHj08aqpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# Implementation of MHA block\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_dim: int=768,\n",
        "               num_heads: int=12,\n",
        "\n",
        "               dropout: float=0.1): #assuming no dropout in MHA block as ViT paper doesn't mention it\n",
        "      super().__init__()\n",
        "\n",
        "      #Calculate number of patches (height * width/patch^2)\n",
        "      self.embedding_dim = embedding_dim\n",
        "      self.num_heads = num_heads\n",
        "\n",
        "      #Normalization layer - Layer Normalization normalizes activations across feature dimensions within a layer,\n",
        "      #ensuring that no single feature’s scale dominates the others, which stabilizes training and improves gradient flow.\n",
        "      self.norm = nn.LayerNorm(embedding_dim)\n",
        "      self.MultiHeadAttention = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                                      num_heads=num_heads,\n",
        "                                                      dropout=dropout,\n",
        "                                                      batch_first=True) #If True, then the input and output tensors are provided as (batch, seq, feature) --> batch_size, embdded_patch_feature, embedded_feature_dimension\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual_input = x\n",
        "    x = self.norm(x)\n",
        "    x, attn_output_weights = self.MultiHeadAttention(x, x, x)\n",
        "    x = self.dropout(x)\n",
        "    return x + residual_input"
      ],
      "metadata": {
        "id": "BD9j6Gk2aaV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "patched_position_embedded_tensor = torch.randn(1, 197, 768) # (batch_size, patch_seq_length, embedding_dim)\n",
        "\n",
        "MHA_instance = MultiHeadAttention()\n",
        "output_after_MHA_block = MHA_instance(patched_position_embedded_tensor)\n",
        "print(f\"Input shape of MHA block: {patched_position_embedded_tensor.shape}\")\n",
        "print(f\"Output shape MHA block: {output_after_MHA_block.shape}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uYMfMfginZs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this step we will implement MLP Block. Paper Quote: \"The MLP contains two layers with a GELU non-linearity in between.\" Dropout is implied/standard (0.1 rate, as in the full transformer spec).\n",
        "##` MLP = Layer Norm Linear -> Linear (expand) → GELU activation → Dropout → Linear (contract) → Dropout + Output from Multi Head Block (residual connection)`"
      ],
      "metadata": {
        "id": "CKpnYFpUpQ45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# Implementation of MLP block\n",
        "\n",
        "class MLP (nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_dim: int=768,\n",
        "               dropout: float=0.1,\n",
        "               mlp_size: int=3072):\n",
        "      super().__init__()\n",
        "      #Normalization layer - Layer Normalization normalizes activations across feature dimensions within a layer,\n",
        "      #ensuring that no single feature’s scale dominates the others, which stabilizes training and improves gradient flow.\n",
        "      self.norm = nn.LayerNorm(embedding_dim)\n",
        "      self.mlp_layer = nn.Sequential(nn.Linear(embedding_dim, mlp_size),\n",
        "                                     nn.GELU(),\n",
        "                                     nn.Dropout(dropout),\n",
        "                                     nn.Linear(mlp_size, embedding_dim))\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual_input = x\n",
        "    x = self.norm(x)\n",
        "    x = self.mlp_layer(x)\n",
        "    x = self.dropout(x)\n",
        "    return x + residual_input\n"
      ],
      "metadata": {
        "id": "WRkiDndHsKVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Testing\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "output_after_MHA_block = torch.randn(1, 197, 768) # (batch_size, patch_seq_length, embedding_dim)\n",
        "\n",
        "MLP_instance = MLP()\n",
        "output_after_MLP_block = MHA_instance(output_after_MHA_block)\n",
        "print(f\"Input shape of MLP block: {output_after_MHA_block.shape}\")\n",
        "print(f\"Output shape MLPblock: {output_after_MLP_block.shape}\")"
      ],
      "metadata": {
        "id": "g2HA42ulvDZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In this step we will create Encoder Layer combinind our above two blocks `MultiHeadAttention` and `MLP`"
      ],
      "metadata": {
        "id": "0vKFqOVswO_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embedding_dim: int=768,\n",
        "                 num_heads: int=12,\n",
        "                 mlp_size: int=3072,\n",
        "                 dropout: float=0.1):\n",
        "        super().__init__()\n",
        "        self.mha_block = MultiHeadAttention(embedding_dim, num_heads, dropout)\n",
        "        self.mlp_block = MLP(embedding_dim, dropout,mlp_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Sequential: Attention then MLP\n",
        "        x = self.mha_block(x)\n",
        "        x = self.mlp_block(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "y3Uc_wEZwmg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Testing\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "patched_position_embedded_tensor = torch.randn(1, 197, 768) # (batch_size, patch_seq_length, embedding_dim)\n",
        "\n",
        "encoder = EncoderLayer()\n",
        "output_after_encoder_layer  = encoder(patched_position_embedded_tensor)\n",
        "print(f\"Input shape to Encoder Layer: {patched_position_embedded_tensor.shape}\")\n",
        "print(f\"Output shape after Encoder Layer: {output_after_encoder_layer.shape}\")"
      ],
      "metadata": {
        "id": "3LWiBQR1wymq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Lets re-implement Vit with our custom made Encoder Layer"
      ],
      "metadata": {
        "id": "5xXt4sj4yZc1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v_56KItdyfXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mBBAKkDyle_"
      },
      "outputs": [],
      "source": [
        "# TODO: Ankit\n",
        "class ViT_01 (nn.Module):\n",
        "  def __init__ (self,\n",
        "                img_size: int=224,\n",
        "                patch_size: int=16,\n",
        "                in_channels: int=3,\n",
        "                mlp_size: int=3072,\n",
        "                num_heads: int=12,\n",
        "                num_encooder_layers: int=12,\n",
        "                embedding_dim: int=768,\n",
        "                attn_dropout: float=0.1,\n",
        "                mlp_dropout: float=0.1,\n",
        "                embedding_dropout: float=0.1,\n",
        "                num_out_class: int=1000):\n",
        "    super().__init__()\n",
        "\n",
        "    assert img_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "    self.num_patches = int((img_size // patch_size) ** 2)\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "    self.embedding_class = nn.Parameter(torch.zeros(1, 1, embedding_dim))\n",
        "    self.embedding_position = nn.Parameter(torch.zeros(1, 1 + self.num_patches, embedding_dim))\n",
        "    self.embedding_dropout = nn.Dropout(embedding_dropout)\n",
        "    self.patch = nn.Conv2d(in_channels=in_channels,\n",
        "                                    out_channels=embedding_dim,\n",
        "                                    kernel_size=patch_size,\n",
        "                                    stride=patch_size)\n",
        "    self.flatten_patches = nn.Flatten(start_dim=2, end_dim=3)\n",
        "\n",
        "    # Custom encoder stack (ModuleList for explicit layering)\n",
        "    self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(\n",
        "                embedding_dim=embedding_dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_size=mlp_size\n",
        "            )\n",
        "            for _ in range(num_encooder_layers)\n",
        "        ])\n",
        "\n",
        "    self.classifier = nn.Sequential(nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "                                    nn.Linear(in_features=embedding_dim, out_features=num_out_class)\n",
        "                                    )\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    x = self.patch(x)\n",
        "    x = self.flatten_patches(x)\n",
        "    x = x.permute(0, 2, 1)\n",
        "    class_embedding = self.embedding_class.expand(batch_size, -1, -1)\n",
        "    x = torch.cat([class_embedding, x], dim=1)\n",
        "    x = self.embedding_position + x\n",
        "    x = self.embedding_dropout(x)\n",
        "    # Encoder stack (FIX: Loop over layers)\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "            x = encoder_layer(x)\n",
        "    x = self.classifier(x[:, 0])\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)\n",
        "vit = ViT_01(num_out_class=3)\n",
        "vit(random_image_tensor)"
      ],
      "metadata": {
        "id": "kebA5z973tUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to get torchinfo, install it if it doesn't work\n",
        "!pip install -q torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "# # Print a summary of our custom ViT model using torchinfo (uncomment for actual output)\n",
        "summary(model=vit,\n",
        "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "id": "z7o2ic4x4kn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "results = engine.train(model=vit,\n",
        "                       train_dataloader=train_dataloader,\n",
        "                       test_dataloader=test_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       loss_fn=loss_fn,\n",
        "                       epochs=10,\n",
        "                       device=device)"
      ],
      "metadata": {
        "id": "U0g64LfN4_OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(results)"
      ],
      "metadata": {
        "id": "IYLUU7nj6HYF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}