{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeq6KeM6TN4z/e1oiv7+e4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitgoelcmu/DeepLearning/blob/main/DeepLearning_Concepts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RNNs\n",
        "\n",
        "\n",
        "1.   Prcoess one word at a time\n",
        "2.   Hidden state from current state is passed on to the next\n",
        "3.   Lacks long term memory, use case static word embedding, lacks full context of the sentences or given input due to sequentually processing of words\n",
        "4. RNNs suffer from Vanishing Gradient Issue -\n",
        "5. Memory context is limited and causes it to forget earlier parts of long sentences.\n",
        "5. Sequential processing also slows down training and inerence as each word output depends on previous one\n",
        "\n",
        "\n",
        "## What is vanishing gradient problem\n",
        "The vanishing gradient problem in RNNs happens when gradients, calculated during backpropagation through time, become extremely small, preventing early layers (time steps) from learning long-term dependencies in sequences because their weight updates become negligible, effectively halting learning for long-range connections, often due to repeated multiplication of small derivative values from activation functions like tanh. This makes standard RNNs struggle to remember information from far back in the input sequence, leading to poor performance on tasks requiring long-term memory.\n",
        "\n",
        "\n",
        "## Why it happens in RNNs\n",
        "1. Backpropagation Through Time (BPTT): RNNs process sequences step-by-step, and during training (BPTT), the error signal travels back through many time steps.\n",
        "2. Chain Rule Multiplication: Gradients are calculated by multiplying derivatives from each time step.\n",
        "\n",
        "2. Activation Function Gradients: Activation functions like tanh or sigmoid have derivatives that are often less than 1 (e.g., tanh derivatives are between 0 and 1).\n",
        "\n",
        "3. Exponential Shrinking: Multiplying many numbers less than 1 together causes the product to shrink exponentially, making the gradient for early time steps tiny.\n",
        "\n",
        "##Consequences\n",
        "\n",
        "1. Slow/Halted Learning: Early weights don't get updated significantly, so the network can't learn patterns or context from the beginning of long sequences.\n",
        "2. Inability to Capture Long-Term Dependencies: The model fails to connect events far apart in the sequence, a key challenge for standard RNNs\n"
      ],
      "metadata": {
        "id": "bZ9oR_DXr9R1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vRx8F6hFue54"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QdIdA-gsueIb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}